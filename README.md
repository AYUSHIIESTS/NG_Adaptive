As a Research Intern under SRFP 2025 at IISc Bangalore, I explored the Neural Galerkin method to solve high-dimensional nonlinear PDEs by integrating deep learning with the classical Galerkin approach. Traditional numerical methods struggle with scalability due to the curse of dimensionality, but the Neural Galerkin framework overcomes this by replacing fixed basis functions with trainable neural networks whose parameters evolve over time, guided by the Dirac-Frenkel variational principle. I implemented both shallow architectures with Gaussian activations and deep networks with sine-based tanh layers to represent solutions of PDEs such as the Advection, Allenâ€“Cahn, and KdV equations. Using JAX, I encoded these time-varying networks, computed residuals to update the system dynamically, and captured phenomena like sharp fronts, phase interfaces, and solitons. I incorporated adaptive sampling and time-stepping strategies to accelerate convergence and improve generalization. I further extended the methodology to formulate and represent solutions for second-order PDEs, pushing the boundaries of this approach towards more complex dynamical systems.

